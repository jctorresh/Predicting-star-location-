{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# working with continuous features\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_file(ss,file_path):\n",
    "    reader=ss.read\n",
    "    reader.option(\"header\",True)\n",
    "    reader.option(\"inferSchema\",True)\n",
    "    reader.option(\"sep\",\"\\t\")\n",
    "    reader.option(\"comment\",\"#\")\n",
    "    df = reader.csv(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_plx(df):\n",
    "    # get rid of second row (string values)\n",
    "    df1 = df.filter(df.Plx!=\"mas\").select(\"Plx\", \"e_Plx\",\"Gmag\",\"GLON\",\"GLAT\",\"Teff\")\n",
    "    # cast Plx, e_Plx, Gmag, Teff columns to floats\n",
    "    df2=df1.select(df1.Plx.cast(\"Float\"), df1.e_Plx.cast(\"Float\"),df1.Gmag.cast(\"Float\"),df1.GLON,df1.GLAT,df1.Teff.cast(\"Float\"))\n",
    "    # keep only positive parallax values\n",
    "    positive_plxs = df2.filter(df2.Plx>0)\n",
    "    return positive_plxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_plx_filt(df, percnt):\n",
    "    # keep top 10% parallax measurements\n",
    "    plx_10perc=df.filter(col(\"e_Plx\")/col(\"Plx\") <percnt)\n",
    "    # now find distance in parsecs by doing 1/(Plx *10^-3)\n",
    "    dist_top10perc = plx_10perc.withColumn(\"Dist\", 1/(plx_10perc.Plx*10**-3))\n",
    "    return dist_top10perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_Teffs(df,Teff_cntCutoff):\n",
    "    # all Teff values\n",
    "    Teff_all = df.select(\"Teff\").rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # first group df by Teff and get count for each Teff\n",
    "    Teff_grps=df.groupBy(\"Teff\").agg(count(\"Teff\").alias(\"Teff_count\"))\n",
    "    \n",
    "    Teff_common=Teff_grps.filter(Teff_grps.Teff_count >Teff_cntCutoff)\n",
    "    # make list of above common Teff values \n",
    "    Teff_comList= Teff_common.select(\"Teff\").rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # make dataframe of only this common Teff values\n",
    "    Teff_com_df=df.where(df.Teff.isin(Teff_comList))\n",
    "    # drop duplicates and only keep one\n",
    "    Teff_no_dups=Teff_com_df.drop_duplicates([\"Teff\"])\n",
    "    \n",
    "    # making dataframe of not common Teff values \n",
    "    Teff_not_comList= list(set(Teff_all)-set(Teff_comList))\n",
    "    Teff_not_comList_df = df.where(df.Teff.isin(Teff_not_comList))\n",
    "    \n",
    "    \n",
    "    # putting them together\n",
    "    Teff_final_df = Teff_not_comList_df.union(Teff_no_dups)\n",
    "    return Teff_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmag_dist_in(df):\n",
    "    # 8000 minus Distance column to give distance from center of galaxy\n",
    "    # because the Dist column is for distance from us to galaxy center\n",
    "    \n",
    "    dist_cent=df.withColumn(\"Dist_from_center\",8000-df.Dist)\n",
    "    \n",
    "    # use the Dist column to find absilute magnitude of star as seen from earth\n",
    "    # M = m - 5*logBase10(d) -5\n",
    "    df_AbsGmag=dist_cent.withColumn(\"Gmag_absolute\", dist_cent.Gmag-5*log(10.0, dist_cent.Dist) -5) \n",
    "    \n",
    "    return df_AbsGmag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_buckets_labels(df,first_bin,last_bin,bin_width):\n",
    "    # bucket bins\n",
    "    bucketBoders=list(np.arange(first_bin,last_bin,bin_width))\n",
    "    # how to set splits for buckets\n",
    "    bucketer = Bucketizer().setSplits(bucketBoders).setInputCol(\"Dist_from_center\").setOutputCol(\"bucketed_distances\")\n",
    "    # distance buckets made using the df column \"Dist_from_center\"\n",
    "    dist_buckets=bucketer.transform(df.select(\"Dist_from_center\"))\n",
    "    \n",
    "    # add this colum of bucketized dataframe to original frame\n",
    "    df_finally=df.join(dist_buckets, df.Dist_from_center == dist_buckets.Dist_from_center,how='left') \n",
    "\n",
    "    return df_finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_columns(df):\n",
    "    # cast to type \"Double\"\n",
    "    new_df=df.select(df.Gmag_absolute.cast(\"Double\"), \"Dist\",df.Teff.cast(\"Double\"))\n",
    "    cols = new_df.select(\"Gmag_absolute\",\"Teff\").columns\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(df,cols):\n",
    "    stages = [] # stages in our Pipeline\n",
    "    # Convert label into label indices using the StringIndexer\n",
    "    label_stringIdx = StringIndexer(inputCol=\"bucketed_distances\", outputCol=\"label\")\n",
    "    stages += [label_stringIdx]\n",
    "    numericCols = [\"Gmag_absolute\",\"Teff\"]\n",
    "    assemblerInputs =  numericCols\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    stages += [assembler]\n",
    "    \n",
    "    partialPipeline = Pipeline().setStages(stages)\n",
    "    pipelineModel=partialPipeline.fit(df.select(df.Gmag_absolute,df.Teff, df.bucketed_distances))\n",
    "    preppedDataDF = pipelineModel.transform(df.select(df.Gmag_absolute,df.Teff, df.bucketed_distances))\n",
    "    \n",
    "    # Keep relevant columns\n",
    "    selectedcols = [\"label\", \"features\"] + cols\n",
    "    dataset = preppedDataDF.select(selectedcols)\n",
    "    \n",
    "    dataset = preppedDataDF.select(selectedcols)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_model(trainData):\n",
    "    # Create initial Decision Tree Model\n",
    "    dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n",
    "\n",
    "    # Train model with Training Data\n",
    "    dtModel = dt.fit(trainData)\n",
    "    return dtModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(testData,dtModel):\n",
    "    # Make predictions on test data using the Transformer.transform() method.\n",
    "    predictions = dtModel.transform(testData)\n",
    "    \n",
    "    # View model's predictions and probabilities of each prediction class\n",
    "    selected = predictions.select(\"label\", \"prediction\", \"probability\", \"Gmag_absolute\", \"Teff\")\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tsv_inGalx = \"asu_withGmag.tsv\" \n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"cs696projgaia\").getOrCreate()\n",
    "    \n",
    "    df_in=get_data_file(spark,tsv_inGalx)\n",
    "    \n",
    "    pos_plxin = pos_plx(df_in)\n",
    "    \n",
    "    topdists_in=dist_plx_filt(pos_plxin, 0.1)\n",
    "    \n",
    "    Teffs_filt_in= filter_Teffs(topdists_in,5)\n",
    "    \n",
    "    \n",
    "    df_final_in=absmag_dist_in(Teffs_filt_in)\n",
    "    \n",
    "    df_final_in.repartition(1).write.format('csv').save('df_i')\n",
    "    \n",
    "    \n",
    "    buckets_dists=dist_buckets_labels(df_final_in,0,8200,200)\n",
    "    \n",
    "    \n",
    "    columns =feature_columns(buckets_dists)\n",
    "    \n",
    "    dataset_prep=pipeline(buckets_dists, columns)\n",
    "    \n",
    "    ### Randomly split data into training and test sets. set seed for reproducibility\n",
    "    (trainingData, testData) = dataset_prep.randomSplit([0.7, 0.3], seed=100)\n",
    "    \n",
    "    dtmodel=decision_tree_model(trainingData)\n",
    "    \n",
    "    df_predic= prediction(testData,dtmodel)\n",
    "    ## due to format of data frame we could output it to csv file\n",
    "    ## we tried multiple things\n",
    "    ## however we are able to it with smaller data set in jupyter notebook\n",
    "\n",
    "\n",
    "    spark.stop()\n",
    "#----------------------------------------------------------------\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
